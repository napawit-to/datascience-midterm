{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 33)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/admin/DataScience/midterm/student/student-mat.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'G2')\n",
    "df = df.drop(columns = 'G1')\n",
    "df = df.drop(columns = 'Mjob')\n",
    "df = df.drop(columns = 'Fjob')\n",
    "df = df.drop(columns = 'Medu')\n",
    "df = df.drop(columns = 'address')\n",
    "df = df.drop(columns = 'Fedu')\n",
    "df = df.drop(columns = 'traveltime')\n",
    "df['sex'].replace(['M', 'F'], [1, 0], inplace=True)\n",
    "# df['address'].replace(['U', 'R'], [1, 0], inplace=True)\n",
    "df['famsize'].replace(['LE3', 'GT3'], [1, 0], inplace=True)\n",
    "df['Pstatus'].replace(['T', 'A'], [1, 0], inplace=True)\n",
    "# df['Mjob'].replace(['other', 'services','at_home','teacher','health'], [4,3,2,1,0], inplace=True)\n",
    "# df['Fjob'].replace(['other', 'services','at_home','teacher','health'], [4,3,2,1,0], inplace=True)\n",
    "df['reason'].replace(['course', 'home','reputation','other'], [3,2,1,0], inplace=True)\n",
    "df['schoolsup'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['famsup'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['paid'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['activities'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['nursery'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['higher'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['internet'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['romantic'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df.loc[df['G3'] < 11, 'G3'] = 0\n",
    "df.loc[df['G3'] > 10, 'G3'] = 1\n",
    "X = df.iloc[:,1:27].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 3]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [4 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [3 0]\n",
      " [4 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [4 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [1 3]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 3]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [2 3]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 3]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [4 0]\n",
      " [3 1]\n",
      " [2 3]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [4 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [4 0]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [4 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [4 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 3]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 3]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X[:,7:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 10] = labelencoder_X_1.fit_transform(X[:, 10])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [10])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selection = X[:,[ 0,2,4,6,7,9,10,15,19,27]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selection, y, test_size=0.2,\n",
    "random_state=30434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "selector_model = selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.65144643e-01 5.48735582e+00 7.56886058e-01 5.26984477e-01\n",
      " 7.00312703e-02 4.58887649e+00 4.76350774e-01 6.92050710e+01\n",
      " 5.74800922e-02 9.96915344e-01]\n"
     ]
    }
   ],
   "source": [
    "print(selector_model.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 10)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model = selector_model.transform(X_train)\n",
    "X_test_model = selector_model.transform(X_test)\n",
    "X_train_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5094339622641509\n",
      " - recall = 0.75\n",
      " - f1 = 0.6067415730337078\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5443037974683544\n",
      " - precision = 0.5\n",
      " - recall = 0.5277777777777778\n",
      " - f1 = 0.5135135135135136\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5128205128205128\n",
      " - recall = 0.5555555555555556\n",
      " - f1 = 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.4968\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.6928 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6922 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.6918 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6911 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6905 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6894 - acc: 0.5475\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6887 - acc: 0.5475\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.6875 - acc: 0.5475\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6864 - acc: 0.5475\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6854 - acc: 0.5475\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6846 - acc: 0.5475\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6829 - acc: 0.5475\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6820 - acc: 0.5475\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6806 - acc: 0.5475\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6796 - acc: 0.5475\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6783 - acc: 0.5475\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6773 - acc: 0.5475\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6764 - acc: 0.5475\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6751 - acc: 0.5475\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - ETA: 0s - loss: 0.6757 - acc: 0.600 - 0s 159us/step - loss: 0.6736 - acc: 0.5475\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6729 - acc: 0.5475\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6716 - acc: 0.5475\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6704 - acc: 0.5475\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6696 - acc: 0.5475\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.6682 - acc: 0.5475\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6675 - acc: 0.5886\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6665 - acc: 0.6171\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.6665 - acc: 0.6266\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6641 - acc: 0.6234\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6638 - acc: 0.6329\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6620 - acc: 0.6392\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6635 - acc: 0.6329\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6611 - acc: 0.6392\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6608 - acc: 0.6392\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6596 - acc: 0.6297\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6587 - acc: 0.6329\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6573 - acc: 0.6329\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6586 - acc: 0.6329\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6579 - acc: 0.6456\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.6563 - acc: 0.6519\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6544 - acc: 0.6424\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6539 - acc: 0.6487\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6531 - acc: 0.6456\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.6529 - acc: 0.6456\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6518 - acc: 0.6456\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6513 - acc: 0.6456\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6498 - acc: 0.6487\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6492 - acc: 0.6551\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6496 - acc: 0.6582\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6480 - acc: 0.6582\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6481 - acc: 0.6582\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6468 - acc: 0.6582\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.6466 - acc: 0.6582\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6460 - acc: 0.6646\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6451 - acc: 0.6582\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6449 - acc: 0.6614\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6437 - acc: 0.6582\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6435 - acc: 0.6551\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6435 - acc: 0.6582\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6426 - acc: 0.6519\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6422 - acc: 0.6582\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6416 - acc: 0.6519\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6416 - acc: 0.6551\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6410 - acc: 0.6456\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6392 - acc: 0.6487\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6391 - acc: 0.6487\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6400 - acc: 0.6519\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6382 - acc: 0.6551\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6378 - acc: 0.6582\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6387 - acc: 0.6519\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.6372 - acc: 0.6582\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6379 - acc: 0.6582\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6364 - acc: 0.6646\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6360 - acc: 0.6519\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6363 - acc: 0.6614\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6386 - acc: 0.6709\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6357 - acc: 0.6519\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6349 - acc: 0.6582\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6344 - acc: 0.6614\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6336 - acc: 0.6677\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6333 - acc: 0.6677\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 155us/step - loss: 0.6329 - acc: 0.6709\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.6336 - acc: 0.6646\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6326 - acc: 0.6709\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6336 - acc: 0.6677\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6324 - acc: 0.6614\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6324 - acc: 0.6709\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6325 - acc: 0.6646\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6328 - acc: 0.6677\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6307 - acc: 0.6741\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6312 - acc: 0.6709\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6296 - acc: 0.6741\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6300 - acc: 0.6741\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6306 - acc: 0.6741\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6298 - acc: 0.6677\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6292 - acc: 0.6804\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6290 - acc: 0.6709\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6284 - acc: 0.6709\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6285 - acc: 0.6614\n",
      "79/79 [==============================] - 0s 5ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.6982690691947937\n",
      "Test accuracy: 0.5696202516555786\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  FORWARD AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "#Set 'forward' to True for forward feature selection\n",
    "sffs = SFS(logisticRegr,\n",
    "k_features=10,\n",
    "forward=True, #For backward selection, set it to False\n",
    "floating=False,\n",
    "verbose=2,\n",
    "scoring='accuracy',\n",
    "cv=10,#10-cross validation\n",
    "n_jobs=-1)#Using all availabel CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    1.2s finished\n",
      "\n",
      "[2019-03-19 21:34:47] Features: 1/10 -- score: 0.6652981427174975[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:47] Features: 2/10 -- score: 0.6749755620723363[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  29 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:48] Features: 3/10 -- score: 0.6844574780058652[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  28 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 21:34:48] Features: 4/10 -- score: 0.6874877810361683[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  27 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:49] Features: 5/10 -- score: 0.690909090909091[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  26 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:49] Features: 6/10 -- score: 0.6939393939393941[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  25 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:50] Features: 7/10 -- score: 0.6941348973607038[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:50] Features: 8/10 -- score: 0.6971652003910069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  23 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:34:50] Features: 9/10 -- score: 0.703225806451613[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  22 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 21:34:51] Features: 10/10 -- score: 0.7066471163245358"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'feature_idx': (15,),\n",
       "  'cv_scores': array([0.63636364, 0.57575758, 0.66666667, 0.67741935, 0.67741935,\n",
       "         0.64516129, 0.77419355, 0.67741935, 0.64516129, 0.67741935]),\n",
       "  'avg_score': 0.6652981427174975,\n",
       "  'feature_names': ('15',)},\n",
       " 2: {'feature_idx': (15, 16),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.6969697 , 0.70967742, 0.74193548,\n",
       "         0.58064516, 0.83870968, 0.64516129, 0.70967742, 0.64516129]),\n",
       "  'avg_score': 0.6749755620723363,\n",
       "  'feature_names': ('15', '16')},\n",
       " 3: {'feature_idx': (15, 16, 20),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.72727273, 0.74193548, 0.77419355,\n",
       "         0.58064516, 0.83870968, 0.64516129, 0.67741935, 0.67741935]),\n",
       "  'avg_score': 0.6844574780058652,\n",
       "  'feature_names': ('15', '16', '20')},\n",
       " 4: {'feature_idx': (7, 15, 16, 20),\n",
       "  'cv_scores': array([0.57575758, 0.63636364, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.61290323, 0.67741935, 0.70967742]),\n",
       "  'avg_score': 0.6874877810361683,\n",
       "  'feature_names': ('7', '15', '16', '20')},\n",
       " 5: {'feature_idx': (7, 15, 16, 20, 22),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.690909090909091,\n",
       "  'feature_names': ('7', '15', '16', '20', '22')},\n",
       " 6: {'feature_idx': (1, 7, 15, 16, 20, 22),\n",
       "  'cv_scores': array([0.57575758, 0.63636364, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6939393939393941,\n",
       "  'feature_names': ('1', '7', '15', '16', '20', '22')},\n",
       " 7: {'feature_idx': (1, 7, 15, 16, 20, 21, 22),\n",
       "  'cv_scores': array([0.54545455, 0.63636364, 0.72727273, 0.77419355, 0.80645161,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6941348973607038,\n",
       "  'feature_names': ('1', '7', '15', '16', '20', '21', '22')},\n",
       " 8: {'feature_idx': (1, 7, 12, 15, 16, 20, 21, 22),\n",
       "  'cv_scores': array([0.60606061, 0.63636364, 0.6969697 , 0.77419355, 0.80645161,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6971652003910069,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '20', '21', '22')},\n",
       " 9: {'feature_idx': (1, 7, 12, 15, 16, 20, 21, 22, 26),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.66666667, 0.77419355, 0.80645161,\n",
       "         0.61290323, 0.77419355, 0.61290323, 0.67741935, 0.77419355]),\n",
       "  'avg_score': 0.703225806451613,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '20', '21', '22', '26')},\n",
       " 10: {'feature_idx': (1, 7, 12, 15, 16, 18, 20, 21, 22, 26),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.63636364, 0.77419355, 0.80645161,\n",
       "         0.61290323, 0.80645161, 0.64516129, 0.67741935, 0.77419355]),\n",
       "  'avg_score': 0.7066471163245358,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '18', '20', '21', '22', '26')}}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs = sffs.fit(X_train,y_train)\n",
    "sffs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7066471163245358"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs.k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5822784810126582\n",
      " - precision = 0.5283018867924528\n",
      " - recall = 0.7777777777777778\n",
      " - f1 = 0.6292134831460674\n"
     ]
    }
   ],
   "source": [
    "X_train_model = sffs.transform(X_train)\n",
    "X_train_model\n",
    "X_test_model = sffs.transform(X_test)\n",
    "X_test_model\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.4936708860759494\n",
      " - precision = 0.45454545454545453\n",
      " - recall = 0.5555555555555556\n",
      " - f1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5063291139240507\n",
      " - precision = 0.47058823529411764\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 3ms/step - loss: 0.6931 - acc: 0.4873\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6926 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6919 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6909 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6892 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6870 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6831 - acc: 0.5823\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6790 - acc: 0.5981\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6737 - acc: 0.6203\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6687 - acc: 0.6297\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6642 - acc: 0.6424\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6580 - acc: 0.6456\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6514 - acc: 0.6424\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6452 - acc: 0.6519\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6401 - acc: 0.6677\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6345 - acc: 0.6614\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6270 - acc: 0.6709\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6218 - acc: 0.6646\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6175 - acc: 0.6772\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.6128 - acc: 0.6835\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6090 - acc: 0.6899\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.6076 - acc: 0.6899\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.6051 - acc: 0.6899\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.6007 - acc: 0.6930\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.6068 - acc: 0.6994\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5992 - acc: 0.6867\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5953 - acc: 0.6930\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5970 - acc: 0.6899\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5946 - acc: 0.6962\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5934 - acc: 0.6994\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5919 - acc: 0.6994\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5922 - acc: 0.6994\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5907 - acc: 0.6962\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5903 - acc: 0.6962\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5901 - acc: 0.6994\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5899 - acc: 0.6930\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5883 - acc: 0.6994\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5887 - acc: 0.7120\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5870 - acc: 0.6994\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5881 - acc: 0.6962\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5885 - acc: 0.6994\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.5867 - acc: 0.7025\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5866 - acc: 0.7120\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5861 - acc: 0.7025\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5877 - acc: 0.7025\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5851 - acc: 0.7184\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5862 - acc: 0.6930\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5860 - acc: 0.7057\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5842 - acc: 0.7057\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5846 - acc: 0.6994\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5842 - acc: 0.7089\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5846 - acc: 0.7025\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 136us/step - loss: 0.5844 - acc: 0.7089\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5832 - acc: 0.7120\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 125us/step - loss: 0.5829 - acc: 0.7120\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.5831 - acc: 0.7089\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5829 - acc: 0.7120\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5850 - acc: 0.6962\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5836 - acc: 0.7152\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5840 - acc: 0.7120\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5827 - acc: 0.7025\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5811 - acc: 0.7120\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5818 - acc: 0.7057\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5818 - acc: 0.7089\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5810 - acc: 0.7089\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5803 - acc: 0.7120\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5817 - acc: 0.7057\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5807 - acc: 0.7120\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5798 - acc: 0.7089\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5793 - acc: 0.7120\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5798 - acc: 0.7152\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5790 - acc: 0.7120\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5794 - acc: 0.7089\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5812 - acc: 0.7089\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5787 - acc: 0.7057\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5787 - acc: 0.7120\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5788 - acc: 0.7120\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5785 - acc: 0.7184\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5798 - acc: 0.7215\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5795 - acc: 0.6962\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5773 - acc: 0.7152\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5785 - acc: 0.7120\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 131us/step - loss: 0.5782 - acc: 0.7184\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5775 - acc: 0.7120\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5775 - acc: 0.7184\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5777 - acc: 0.7152\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5770 - acc: 0.7152\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5773 - acc: 0.7120\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5781 - acc: 0.7184\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5783 - acc: 0.7120\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5775 - acc: 0.7120\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5763 - acc: 0.7057\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5774 - acc: 0.7215\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5753 - acc: 0.7089\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5758 - acc: 0.7120\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5757 - acc: 0.7089\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5757 - acc: 0.7152\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5755 - acc: 0.7120\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5782 - acc: 0.7152\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5757 - acc: 0.7152\n",
      "79/79 [==============================] - 0s 3ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.7033859491348267\n",
      "Test accuracy: 0.5569620132446289\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SBS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Set 'forward' to False for backward feature selection\n",
    "sbfs = SBS(logisticRegr,\n",
    "k_features=10,\n",
    "forward=False, #For backward selection, set it to False\n",
    "floating=False,\n",
    "verbose=2,\n",
    "scoring='accuracy',\n",
    "cv=10,#10-cross validation\n",
    "n_jobs=-1)#Using all availabel CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    4.2s finished\n",
      "\n",
      "[2019-03-19 21:35:28] Features: 30/10 -- score: 0.6689149560117302[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.7s finished\n",
      "\n",
      "[2019-03-19 21:35:32] Features: 29/10 -- score: 0.6789833822091886[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    3.6s finished\n",
      "\n",
      "[2019-03-19 21:35:35] Features: 28/10 -- score: 0.6852394916911047[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    3.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    3.4s finished\n",
      "\n",
      "[2019-03-19 21:35:39] Features: 27/10 -- score: 0.6882697947214076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    3.3s finished\n",
      "\n",
      "[2019-03-19 21:35:42] Features: 26/10 -- score: 0.6882697947214076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    2.7s finished\n",
      "\n",
      "[2019-03-19 21:35:45] Features: 25/10 -- score: 0.6913000977517108[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    2.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    2.6s finished\n",
      "\n",
      "[2019-03-19 21:35:47] Features: 24/10 -- score: 0.70019550342131[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    2.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    2.3s finished\n",
      "\n",
      "[2019-03-19 21:35:50] Features: 23/10 -- score: 0.7068426197458456[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  23 | elapsed:    2.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    2.4s finished\n",
      "\n",
      "[2019-03-19 21:35:52] Features: 22/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  22 | elapsed:    2.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    2.4s finished\n",
      "\n",
      "[2019-03-19 21:35:55] Features: 21/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  21 | elapsed:    1.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    2.1s finished\n",
      "\n",
      "[2019-03-19 21:35:57] Features: 20/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    1.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.9s finished\n",
      "\n",
      "[2019-03-19 21:35:59] Features: 19/10 -- score: 0.7191593352883676[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  19 | elapsed:    1.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    1.8s finished\n",
      "\n",
      "[2019-03-19 21:36:00] Features: 18/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:    1.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.6s finished\n",
      "\n",
      "[2019-03-19 21:36:02] Features: 17/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  17 | elapsed:    1.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    1.1s finished\n",
      "\n",
      "[2019-03-19 21:36:03] Features: 16/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  16 | elapsed:    0.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    1.0s finished\n",
      "\n",
      "[2019-03-19 21:36:04] Features: 15/10 -- score: 0.7159335288367548[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.6s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.9s finished\n",
      "\n",
      "[2019-03-19 21:36:05] Features: 14/10 -- score: 0.7159335288367548[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    0.9s finished\n",
      "\n",
      "[2019-03-19 21:36:06] Features: 13/10 -- score: 0.7187683284457479[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  13 | elapsed:    0.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    0.8s finished\n",
      "\n",
      "[2019-03-19 21:36:07] Features: 12/10 -- score: 0.7155425219941349[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:    0.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 21:36:07] Features: 11/10 -- score: 0.7036168132942328[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.2s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.3s finished\n",
      "\n",
      "[2019-03-19 21:36:07] Features: 10/10 -- score: 0.70019550342131"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{31: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.48484848, 0.61290323, 0.74193548,\n",
       "         0.5483871 , 0.87096774, 0.58064516, 0.67741935, 0.61290323]),\n",
       "  'avg_score': 0.6402737047898339,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '17',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 30: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.58064516, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.64516129, 0.64516129, 0.70967742]),\n",
       "  'avg_score': 0.6689149560117302,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 29: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.48484848, 0.64516129, 0.80645161,\n",
       "         0.64516129, 0.83870968, 0.64516129, 0.70967742, 0.74193548]),\n",
       "  'avg_score': 0.6789833822091886,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 28: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.60606061, 0.63636364, 0.54545455, 0.64516129, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.70967742, 0.74193548]),\n",
       "  'avg_score': 0.6852394916911047,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 27: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.67741935, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6882697947214076,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 26: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.67741935, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6882697947214076,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 25: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.60606061, 0.60606061, 0.70967742, 0.74193548,\n",
       "         0.61290323, 0.83870968, 0.64516129, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.6913000977517108,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 24: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.66666667, 0.66666667, 0.67741935, 0.77419355,\n",
       "         0.64516129, 0.83870968, 0.64516129, 0.74193548, 0.70967742]),\n",
       "  'avg_score': 0.70019550342131,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 23: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.66666667, 0.70967742, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7068426197458456,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 22: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.63636364, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 21: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.63636364, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 20: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.66666667, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 19: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7191593352883676,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 18: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 17: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 16: {'feature_idx': (3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 15: {'feature_idx': (3, 4, 6, 9, 10, 12, 15, 16, 24, 25, 26, 27, 28, 29, 30),\n",
       "  'cv_scores': array([0.6969697 , 0.60606061, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7159335288367548,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 14: {'feature_idx': (3, 4, 6, 9, 10, 12, 15, 16, 24, 25, 26, 27, 28, 30),\n",
       "  'cv_scores': array([0.6969697 , 0.60606061, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7159335288367548,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '30')},\n",
       " 13: {'feature_idx': (3, 4, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28, 30),\n",
       "  'cv_scores': array([0.72727273, 0.63636364, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.61290323, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7187683284457479,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '30')},\n",
       " 12: {'feature_idx': (3, 4, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.72727273, 0.66666667, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.61290323, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7155425219941349,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28')},\n",
       " 11: {'feature_idx': (3, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.66666667, 0.74193548, 0.77419355,\n",
       "         0.58064516, 0.83870968, 0.61290323, 0.74193548, 0.80645161]),\n",
       "  'avg_score': 0.7036168132942328,\n",
       "  'feature_names': ('3',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28')},\n",
       " 10: {'feature_idx': (6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.63636364, 0.66666667, 0.66666667, 0.70967742, 0.74193548,\n",
       "         0.58064516, 0.87096774, 0.61290323, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.70019550342131,\n",
       "  'feature_names': ('6', '9', '12', '15', '16', '24', '25', '26', '27', '28')}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbfs = sbfs.fit(X_train, y_train)\n",
    "sbfs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70019550342131"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbfs.k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      " - accuracy = 0.5822784810126582\n",
      " - precision = 0.5306122448979592\n",
      " - recall = 0.7222222222222222\n",
      " - f1 = 0.611764705882353\n"
     ]
    }
   ],
   "source": [
    "X_train_model = sbfs.transform(X_train)\n",
    "X_test_model = sbfs.transform(X_test)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5189873417721519\n",
      " - precision = 0.4791666666666667\n",
      " - recall = 0.6388888888888888\n",
      " - f1 = 0.5476190476190476\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5111111111111111\n",
      " - recall = 0.6388888888888888\n",
      " - f1 = 0.5679012345679012\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 3ms/step - loss: 0.6932 - acc: 0.4715\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6926 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6919 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6903 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.6883 - acc: 0.5759\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6858 - acc: 0.6076\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6802 - acc: 0.6171\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6728 - acc: 0.6551\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6624 - acc: 0.6487\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.6503 - acc: 0.6519\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6381 - acc: 0.6646\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.6275 - acc: 0.6899\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.6154 - acc: 0.6835\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.6092 - acc: 0.6867\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6017 - acc: 0.6772\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5956 - acc: 0.6772\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5932 - acc: 0.6677\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5866 - acc: 0.6709\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5861 - acc: 0.6741\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5813 - acc: 0.6804\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5802 - acc: 0.6772\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5795 - acc: 0.6709\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5769 - acc: 0.6962\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5751 - acc: 0.6867\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5773 - acc: 0.6930\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5745 - acc: 0.6835\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5733 - acc: 0.6962\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5740 - acc: 0.6741\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5744 - acc: 0.6867\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5713 - acc: 0.6962\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5720 - acc: 0.6899\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5739 - acc: 0.6962\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5735 - acc: 0.6772\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5703 - acc: 0.7057\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.5743 - acc: 0.6930\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5702 - acc: 0.6867\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5687 - acc: 0.6962\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5672 - acc: 0.7057\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5703 - acc: 0.7025\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5730 - acc: 0.6962\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5697 - acc: 0.7025\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5683 - acc: 0.6994\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5676 - acc: 0.6994\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5675 - acc: 0.7025\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5752 - acc: 0.6899\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5697 - acc: 0.6930\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5686 - acc: 0.7057\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5678 - acc: 0.6899\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5681 - acc: 0.6994\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5703 - acc: 0.7025\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5684 - acc: 0.6930\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5669 - acc: 0.7025\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5673 - acc: 0.7057\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5680 - acc: 0.7089\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5669 - acc: 0.6930\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5721 - acc: 0.6867\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5698 - acc: 0.6962\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5675 - acc: 0.7025\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5693 - acc: 0.6899\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5682 - acc: 0.7057\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5662 - acc: 0.7057\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5654 - acc: 0.6930\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5669 - acc: 0.6994\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5662 - acc: 0.6994\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5665 - acc: 0.6962\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5704 - acc: 0.6835\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5698 - acc: 0.7120\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5662 - acc: 0.6930\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5667 - acc: 0.7025\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 133us/step - loss: 0.5710 - acc: 0.6804\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5784 - acc: 0.6994\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5701 - acc: 0.6835\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5666 - acc: 0.6930\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5674 - acc: 0.7025\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5664 - acc: 0.7025\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5667 - acc: 0.6930\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5656 - acc: 0.7025\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5708 - acc: 0.6772\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5677 - acc: 0.7120\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5664 - acc: 0.6994\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5651 - acc: 0.7025\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5670 - acc: 0.6962\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5662 - acc: 0.6994\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5657 - acc: 0.6994\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5661 - acc: 0.6962\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5677 - acc: 0.7120\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5656 - acc: 0.7025\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5672 - acc: 0.7120\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5692 - acc: 0.6994\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5713 - acc: 0.7215\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5666 - acc: 0.7057\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5670 - acc: 0.6899\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5682 - acc: 0.7025\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5660 - acc: 0.6994\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5653 - acc: 0.6994\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5658 - acc: 0.7025\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5691 - acc: 0.6994\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5660 - acc: 0.7120\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5714 - acc: 0.7057\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5639 - acc: 0.7025\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.748385488986969\n",
      "Test accuracy: 0.5822784900665283\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECURSIVE AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "rfe = RFE(estimator=logisticRegr, n_features_to_select=10)\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5949367088607594\n",
      " - precision = 0.54\n",
      " - recall = 0.75\n",
      " - f1 = 0.627906976744186\n"
     ]
    }
   ],
   "source": [
    "X_train_model = rfe.transform(X_train)\n",
    "X_test_model = rfe.transform(X_test)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5121951219512195\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.6075949367088608\n",
      " - precision = 0.5581395348837209\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 3ms/step - loss: 0.6929 - acc: 0.5475\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6925 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6918 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6903 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6873 - acc: 0.5633\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6829 - acc: 0.5886\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6774 - acc: 0.6234\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6685 - acc: 0.6487\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6581 - acc: 0.6392\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6472 - acc: 0.6519\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6379 - acc: 0.6709\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6239 - acc: 0.6741\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6124 - acc: 0.6804\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6029 - acc: 0.6741\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5977 - acc: 0.6835\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5915 - acc: 0.7025\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5882 - acc: 0.6835\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5862 - acc: 0.6741\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5811 - acc: 0.6962\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5812 - acc: 0.6962\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5797 - acc: 0.6804\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5755 - acc: 0.6835\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5756 - acc: 0.6835\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5765 - acc: 0.6772\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5750 - acc: 0.6899\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5732 - acc: 0.7025\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5738 - acc: 0.6867\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5731 - acc: 0.6930\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5695 - acc: 0.6867\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5733 - acc: 0.6835\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5737 - acc: 0.6804\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5694 - acc: 0.6962\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5749 - acc: 0.6582\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.5708 - acc: 0.6835\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5700 - acc: 0.6930\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5712 - acc: 0.6772\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5702 - acc: 0.6772\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.5698 - acc: 0.6835\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5688 - acc: 0.6867\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5680 - acc: 0.6930\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5683 - acc: 0.6930\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5673 - acc: 0.6994\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5692 - acc: 0.6930\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 133us/step - loss: 0.5674 - acc: 0.6930\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5670 - acc: 0.6930\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5667 - acc: 0.6962\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5698 - acc: 0.6962\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5669 - acc: 0.6962\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5671 - acc: 0.6962\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.5676 - acc: 0.6899\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5678 - acc: 0.6930\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5685 - acc: 0.6835\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5683 - acc: 0.6994\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 136us/step - loss: 0.5672 - acc: 0.6835\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5662 - acc: 0.6930\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5708 - acc: 0.6835\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5695 - acc: 0.6804\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5645 - acc: 0.6962\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5663 - acc: 0.6930\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5650 - acc: 0.6994\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5652 - acc: 0.6962\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5658 - acc: 0.6994\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5673 - acc: 0.6804\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5669 - acc: 0.6994\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5649 - acc: 0.6962\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5680 - acc: 0.6835\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5672 - acc: 0.6899\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5660 - acc: 0.6835\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5697 - acc: 0.6582\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5649 - acc: 0.6804\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5653 - acc: 0.6899\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5647 - acc: 0.6962\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5647 - acc: 0.6899\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5642 - acc: 0.6962\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5657 - acc: 0.6835\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5643 - acc: 0.7025\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5634 - acc: 0.6962\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5645 - acc: 0.6899\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5648 - acc: 0.6867\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5688 - acc: 0.6930\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.5670 - acc: 0.6772\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5654 - acc: 0.6804\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5635 - acc: 0.6835\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5643 - acc: 0.6930\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5636 - acc: 0.6835\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5658 - acc: 0.6835\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5652 - acc: 0.6804\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5635 - acc: 0.6867\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5644 - acc: 0.6930\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5636 - acc: 0.6962\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5628 - acc: 0.6930\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5629 - acc: 0.6899\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5647 - acc: 0.6899\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5652 - acc: 0.6867\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5625 - acc: 0.6867\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5642 - acc: 0.6994\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5672 - acc: 0.6804\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5676 - acc: 0.6709\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5632 - acc: 0.6962\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5644 - acc: 0.6930\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.729767382144928\n",
      "Test accuracy: 0.607594907283783\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Methods (Tree-Based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=30434, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01380813 0.01353761 0.00723804 0.02454823 0.04799143 0.01357749\n",
      " 0.02113829 0.00928241 0.04662204 0.04653403 0.05033548 0.03596542\n",
      " 0.03949406 0.02788433 0.04028098 0.08653448 0.02094389 0.01745103\n",
      " 0.02138969 0.0164613  0.01315435 0.01147229 0.01673737 0.0196627\n",
      " 0.03942599 0.05573863 0.05791559 0.02723297 0.04968925 0.04343057\n",
      " 0.06452194]\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=30434, verbose=0,\n",
       "            warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=0.04)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "sfm = SelectFromModel(rf, threshold=0.04)\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False, False, False,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feature_boolean_selector = sfm.get_support()\n",
    "rf_feature_boolean_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model = sfm.transform(X_train)\n",
    "X_test_model = sfm.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " >accuracy = 0.569620253164557\n",
      " >precision = 0.52\n",
      " >recall = 0.7222222222222222\n",
      " >f1 = 0.6046511627906976\n"
     ]
    }
   ],
   "source": [
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" >accuracy = \" + str(accuracy))\n",
    "print(\" >precision = \" + str(precision))\n",
    "print(\" >recall = \" + str(recall))\n",
    "print(\" >f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.569620253164557\n",
      " - precision = 0.525\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5316455696202531\n",
      " - precision = 0.4883720930232558\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5316455696202531\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 4ms/step - loss: 0.6929 - acc: 0.5443\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6924 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6919 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6914 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6906 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6897 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6888 - acc: 0.5475\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6874 - acc: 0.5475\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6857 - acc: 0.5475\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6840 - acc: 0.5475\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6824 - acc: 0.5475\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6801 - acc: 0.5475\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6787 - acc: 0.5475\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6765 - acc: 0.5475\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6743 - acc: 0.5475\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6725 - acc: 0.5475\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.6718 - acc: 0.5475\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.6693 - acc: 0.5475\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 136us/step - loss: 0.6680 - acc: 0.5475\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.6668 - acc: 0.5475\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 136us/step - loss: 0.6651 - acc: 0.5475\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 132us/step - loss: 0.6633 - acc: 0.5475\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.6609 - acc: 0.6139\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.6579 - acc: 0.6297\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6542 - acc: 0.6487\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6488 - acc: 0.6551\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6432 - acc: 0.6614\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6393 - acc: 0.6614\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6363 - acc: 0.6582\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6323 - acc: 0.6614\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6260 - acc: 0.6646\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6241 - acc: 0.6551\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6201 - acc: 0.6772\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6155 - acc: 0.6646\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6126 - acc: 0.6582\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.6112 - acc: 0.6741\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.6091 - acc: 0.6551\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.6073 - acc: 0.6772\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6056 - acc: 0.6646\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6037 - acc: 0.6582\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6055 - acc: 0.6646\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6017 - acc: 0.6677\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6023 - acc: 0.6646\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6016 - acc: 0.6646\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.6097 - acc: 0.6297\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5982 - acc: 0.6551\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5988 - acc: 0.6741\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5962 - acc: 0.6741\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6009 - acc: 0.6424\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5959 - acc: 0.6677\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5993 - acc: 0.6709\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5987 - acc: 0.6551\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5956 - acc: 0.6677\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5959 - acc: 0.6582\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 136us/step - loss: 0.5957 - acc: 0.6646\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.6014 - acc: 0.6677\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5954 - acc: 0.6677\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5947 - acc: 0.6614\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5920 - acc: 0.6677\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5934 - acc: 0.6677\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5930 - acc: 0.6677\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5930 - acc: 0.6519\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5916 - acc: 0.6614\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5914 - acc: 0.6772\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.5936 - acc: 0.6614\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5926 - acc: 0.6614\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5930 - acc: 0.6741\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5913 - acc: 0.6677\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5919 - acc: 0.6582\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5946 - acc: 0.6709\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5933 - acc: 0.6614\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5898 - acc: 0.6772\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5902 - acc: 0.6646\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5894 - acc: 0.6741\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5917 - acc: 0.6709\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5904 - acc: 0.6709\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5898 - acc: 0.6741\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.5902 - acc: 0.6582\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5946 - acc: 0.6677\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5893 - acc: 0.6646\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.5903 - acc: 0.6772\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5891 - acc: 0.6709\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5920 - acc: 0.6582\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 131us/step - loss: 0.5889 - acc: 0.6772\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5919 - acc: 0.6772\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5891 - acc: 0.6709\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5895 - acc: 0.6646\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.5889 - acc: 0.6867\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5894 - acc: 0.6551\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5910 - acc: 0.6709\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5899 - acc: 0.6677\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 134us/step - loss: 0.5885 - acc: 0.6677\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5902 - acc: 0.6646\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 128us/step - loss: 0.5914 - acc: 0.6741\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5907 - acc: 0.6646\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5896 - acc: 0.6677\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5899 - acc: 0.6677\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5890 - acc: 0.6741\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5883 - acc: 0.6677\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5883 - acc: 0.6646\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.7188560962677002\n",
      "Test accuracy: 0.607594907283783\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 11, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=11)\n",
    "pca_selector= pca.fit(X_train,y_train)\n",
    "X_train_model = pca_selector.transform(X_train)\n",
    "X_test_model = pca_selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " >accuracy = 0.4430379746835443\n",
      " >precision = 0.43333333333333335\n",
      " >recall = 0.7222222222222222\n",
      " >f1 = 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" >accuracy = \" + str(accuracy))\n",
    "print(\" >precision = \" + str(precision))\n",
    "print(\" >recall = \" + str(recall))\n",
    "print(\" >f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5189873417721519\n",
      " - precision = 0.48\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5443037974683544\n",
      " - precision = 0.5\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 1s 3ms/step - loss: 0.6929 - acc: 0.5538\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6923 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6915 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.6902 - acc: 0.5506\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.6885 - acc: 0.5601\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6857 - acc: 0.5918\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6817 - acc: 0.6044\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.6770 - acc: 0.6203\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6708 - acc: 0.6171\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6643 - acc: 0.6203\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6572 - acc: 0.6139\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6505 - acc: 0.6234\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6444 - acc: 0.6234\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6396 - acc: 0.6203\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.6345 - acc: 0.6266\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.6312 - acc: 0.6266\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.6257 - acc: 0.6297\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.6232 - acc: 0.6361\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.6200 - acc: 0.6329\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6173 - acc: 0.6392\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6162 - acc: 0.6456\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.6126 - acc: 0.6424\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6107 - acc: 0.6392\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6096 - acc: 0.6392\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6074 - acc: 0.6456\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.6054 - acc: 0.6487\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6039 - acc: 0.6487\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6024 - acc: 0.6487\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 154us/step - loss: 0.6001 - acc: 0.6519\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5993 - acc: 0.6519\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5981 - acc: 0.6614\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5957 - acc: 0.6551\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5948 - acc: 0.6614\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5934 - acc: 0.6709\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 138us/step - loss: 0.5920 - acc: 0.6709\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 142us/step - loss: 0.5910 - acc: 0.6709\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5901 - acc: 0.6772\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5882 - acc: 0.6741\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5882 - acc: 0.6709\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 140us/step - loss: 0.5868 - acc: 0.6709\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5859 - acc: 0.6709\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5851 - acc: 0.6741\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5834 - acc: 0.6772\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5824 - acc: 0.6741\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5826 - acc: 0.6709\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5813 - acc: 0.6804\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5801 - acc: 0.6772\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5788 - acc: 0.6804\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5782 - acc: 0.6772\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5787 - acc: 0.6741\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5767 - acc: 0.6772\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5776 - acc: 0.6804\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5749 - acc: 0.6772\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 0.5748 - acc: 0.6835\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5747 - acc: 0.6835\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5748 - acc: 0.6804\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5724 - acc: 0.6867\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.5717 - acc: 0.6962\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5711 - acc: 0.6899\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5713 - acc: 0.6899\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5716 - acc: 0.6962\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5690 - acc: 0.6962\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5696 - acc: 0.6930\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5683 - acc: 0.6899\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5671 - acc: 0.7057\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5674 - acc: 0.7025\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5669 - acc: 0.6994\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5672 - acc: 0.6962\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5663 - acc: 0.7025\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 146us/step - loss: 0.5659 - acc: 0.6994\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5647 - acc: 0.6994\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5643 - acc: 0.6994\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5649 - acc: 0.7057\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5631 - acc: 0.7057\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 144us/step - loss: 0.5656 - acc: 0.7057\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5628 - acc: 0.7025\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5624 - acc: 0.7025\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5628 - acc: 0.7057\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 141us/step - loss: 0.5607 - acc: 0.7025\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5608 - acc: 0.7089\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5611 - acc: 0.7025\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5599 - acc: 0.7057\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 154us/step - loss: 0.5599 - acc: 0.7057\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5585 - acc: 0.7057\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5589 - acc: 0.7152\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5584 - acc: 0.7089\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 143us/step - loss: 0.5569 - acc: 0.7057\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5568 - acc: 0.7089\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5561 - acc: 0.7025\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5555 - acc: 0.6962\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5553 - acc: 0.7152\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.5575 - acc: 0.7025\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 0.5519 - acc: 0.7089\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 0.5551 - acc: 0.7057\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5526 - acc: 0.7025\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5520 - acc: 0.7120\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 0.5509 - acc: 0.7057\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5531 - acc: 0.7025\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5482 - acc: 0.7089\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.5521 - acc: 0.7057\n",
      "79/79 [==============================] - 0s 4ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.8213231563568115\n",
      "Test accuracy: 0.5696202516555786\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 11, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
