{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 33)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/admin/DataScience/midterm/student/student-mat.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'G2')\n",
    "df = df.drop(columns = 'G1')\n",
    "df.shape\n",
    "df['sex'].replace(['M', 'F'], [1, 0], inplace=True)\n",
    "df['address'].replace(['U', 'R'], [1, 0], inplace=True)\n",
    "df['famsize'].replace(['LE3', 'GT3'], [1, 0], inplace=True)\n",
    "df['Pstatus'].replace(['T', 'A'], [1, 0], inplace=True)\n",
    "df['Mjob'].replace(['other', 'services','at_home','teacher','health'], [4,3,2,1,0], inplace=True)\n",
    "df['Fjob'].replace(['other', 'services','at_home','teacher','health'], [4,3,2,1,0], inplace=True)\n",
    "df['reason'].replace(['course', 'home','reputation','other'], [3,2,1,0], inplace=True)\n",
    "df['schoolsup'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['famsup'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['paid'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['activities'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['nursery'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['higher'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['internet'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df['romantic'].replace(['yes', 'no'], [1, 0], inplace=True)\n",
    "df.loc[df['G3'] < 11, 'G3'] = 0\n",
    "df.loc[df['G3'] > 10, 'G3'] = 1\n",
    "X = df.iloc[:,1:30].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1 4 3 4 2 4 3 3 4 2 4 4 2 4 4 3 3 4 4 4 4 2 2 2 2 4 3 4 4 4 4 3 3 2 4\n",
      " 4 3 2 2 4 4 2 2 4 3 4 4 4 2 4 4 4 3 2 4 4 1 4 4 1 1 4 4 4 4 3 2 3 3 4 1 3\n",
      " 3 4 4 2 2 3 2 2 3 2 1 4 2 4 2 4 3 4 3 4 2 1 4 2 4 4 4 4 4 3 3 3 2 3 4 4 4\n",
      " 3 2 4 2 4 4 3 1 3 1 2 2 4 2 3 3 0 2 4 3 1 2 3 3 4 3 3 1 4 4 2 4 1 2 1 3 1\n",
      " 4 2 1 2 3 3 4 2 1 1 2 3 2 3 1 1 1 3 2 4 2 4 3 1 4 1 3 4 2 3 4 4 4 3 2 3 3\n",
      " 3 1 2 3 1 2 1 1 3 2 2 4 3 4 4 4 2 1 2 2 3 3 4 1 4 3 4 2 2 4 3 4 3 2 2 2 1\n",
      " 2 2 4 3 3 2 2 2 4 2 4 4 1 3 2 2 2 2 4 4 4 4 2 2 2 3 3 0 3 3 2 2 2 1 4 4 2\n",
      " 2 4 4 3 3 2 3 3 4 4 2 3 2 1 1 2 2 3 4 4 4 4 3 1 1 2 1 2 1 2 4 4 4 2 3 3 3\n",
      " 4 4 4 4 4 4 4 3 3 2 3 4 3 1 1 2 1 3 1 2 2 4 3 4 4 2 2 3 0 4 3 2 4 4 2 2 3\n",
      " 2 2 3 3 3 3 3 2 4 3 2 2 3 4 4 4 3 1 3 1 1 4 3 4 3 1 1 1 1 3 4 1 1 4 1 2 4\n",
      " 3 1 2 1 4 1 4 4 3 3 4 2 2 1 4 2 4 2 3 1 2 3 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 10] = labelencoder_X_1.fit_transform(X[:, 10])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [10])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.557856   -0.28017547 -1.2966679  ... -0.42969219 -1.\n",
      "   0.59075423]\n",
      " [-0.557856   -0.28017547 -1.2966679  ... -0.42969219  1.\n",
      "  -0.55451164]\n",
      " [-0.557856   -0.28017547 -0.51619732 ... -0.42969219  1.\n",
      "   1.73602009]\n",
      " ...\n",
      " [-0.557856   -0.28017547  1.04474385 ... -0.42969219  1.\n",
      "   2.88128595]\n",
      " [-0.557856   -0.28017547  1.04474385 ...  2.31339333  1.\n",
      "  -0.55451164]\n",
      " [-0.557856   -0.28017547 -1.2966679  ... -0.42969219 -1.\n",
      "   0.59075423]]\n"
     ]
    }
   ],
   "source": [
    "X_selection = X[:,[ 0,2,4,6,7,9,10,15,19,27]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selection, y, test_size=0.2,\n",
    "random_state=30434)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTER AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9860778c22cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mselector_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "selector_model = selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.65144643e-01 5.48735582e+00 7.56886058e-01 5.26984477e-01\n",
      " 7.00312703e-02 4.58887649e+00 4.76350774e-01 6.92050710e+01\n",
      " 5.74800922e-02 9.96915344e-01]\n"
     ]
    }
   ],
   "source": [
    "print(selector_model.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 10)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model = selector_model.transform(X_train)\n",
    "X_test_model = selector_model.transform(X_test)\n",
    "X_train_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5094339622641509\n",
      " - recall = 0.75\n",
      " - f1 = 0.6067415730337078\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5443037974683544\n",
      " - precision = 0.5\n",
      " - recall = 0.5277777777777778\n",
      " - f1 = 0.5135135135135136\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5128205128205128\n",
      " - recall = 0.5555555555555556\n",
      " - f1 = 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 5ms/step - loss: 0.6931 - acc: 0.4684\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6924 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6917 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6908 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6897 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6883 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.6870 - acc: 0.5475\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.6858 - acc: 0.5475\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.6833 - acc: 0.5475\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6817 - acc: 0.5475\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.6787 - acc: 0.5538\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 207us/step - loss: 0.6756 - acc: 0.5854\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6719 - acc: 0.6076\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.6675 - acc: 0.6139\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6632 - acc: 0.6297\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 173us/step - loss: 0.6583 - acc: 0.6234\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.6523 - acc: 0.6329\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6489 - acc: 0.6329\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6448 - acc: 0.6392\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6396 - acc: 0.6487\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6353 - acc: 0.6487\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.6295 - acc: 0.6519\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 208us/step - loss: 0.6295 - acc: 0.6519\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6273 - acc: 0.6614\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6208 - acc: 0.6551\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.6183 - acc: 0.6519\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.6175 - acc: 0.6519\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.6182 - acc: 0.6551\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 220us/step - loss: 0.6158 - acc: 0.6677\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 209us/step - loss: 0.6127 - acc: 0.6551\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.6135 - acc: 0.6487\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6120 - acc: 0.6551\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6109 - acc: 0.6519\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.6107 - acc: 0.6614\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.6138 - acc: 0.6551\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6140 - acc: 0.6551\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.6089 - acc: 0.6582\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6094 - acc: 0.6582\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.6090 - acc: 0.6646\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.6096 - acc: 0.6646\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.6084 - acc: 0.6519\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6075 - acc: 0.6519\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 221us/step - loss: 0.6073 - acc: 0.6646\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 207us/step - loss: 0.6073 - acc: 0.6614\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.6090 - acc: 0.6709\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.6081 - acc: 0.6646\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 205us/step - loss: 0.6062 - acc: 0.6646\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.6068 - acc: 0.6614\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 207us/step - loss: 0.6091 - acc: 0.6646\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.6071 - acc: 0.6709\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.6073 - acc: 0.6646\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 212us/step - loss: 0.6072 - acc: 0.6614\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6065 - acc: 0.6614\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6088 - acc: 0.6646\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.6061 - acc: 0.6772\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.6074 - acc: 0.6582\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6062 - acc: 0.6677\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.6092 - acc: 0.6551\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6072 - acc: 0.6741\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6066 - acc: 0.6709\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.6063 - acc: 0.6551\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.6058 - acc: 0.6582\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6059 - acc: 0.6646\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.6072 - acc: 0.6677\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.6112 - acc: 0.6614\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.6076 - acc: 0.6646\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6064 - acc: 0.6614\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.6054 - acc: 0.6677\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.6056 - acc: 0.6709\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.6058 - acc: 0.6646\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.6058 - acc: 0.6772\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.6070 - acc: 0.6677\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.6104 - acc: 0.6646\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6057 - acc: 0.6646\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6045 - acc: 0.6677\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6085 - acc: 0.6646\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6064 - acc: 0.6551\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6049 - acc: 0.6741\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.6067 - acc: 0.6741\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 209us/step - loss: 0.6061 - acc: 0.6677\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 210us/step - loss: 0.6065 - acc: 0.6741\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.6056 - acc: 0.6582\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 210us/step - loss: 0.6059 - acc: 0.6677\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 205us/step - loss: 0.6075 - acc: 0.6741\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.6060 - acc: 0.6772\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6073 - acc: 0.6646\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6066 - acc: 0.6646\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.6053 - acc: 0.6646\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6071 - acc: 0.6709\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6078 - acc: 0.6677\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 218us/step - loss: 0.6048 - acc: 0.6741\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.6058 - acc: 0.6772\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.6050 - acc: 0.6709\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6054 - acc: 0.6646\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.6050 - acc: 0.6677\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6053 - acc: 0.6646\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.6055 - acc: 0.6646\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6056 - acc: 0.6677\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6051 - acc: 0.6772\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6068 - acc: 0.6677\n",
      "79/79 [==============================] - 1s 7ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.7252544164657593\n",
      "Test accuracy: 0.5569620132446289\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  FORWARD AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "#Set 'forward' to True for forward feature selection\n",
    "sffs = SFS(logisticRegr,\n",
    "k_features=10,\n",
    "forward=True, #For backward selection, set it to False\n",
    "floating=False,\n",
    "verbose=2,\n",
    "scoring='accuracy',\n",
    "cv=10,#10-cross validation\n",
    "n_jobs=-1)#Using all availabel CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  31 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:47] Features: 1/10 -- score: 0.6652981427174975[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:48] Features: 2/10 -- score: 0.6749755620723363[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  29 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:48] Features: 3/10 -- score: 0.6844574780058652[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  28 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:49] Features: 4/10 -- score: 0.6874877810361683[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  27 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:49] Features: 5/10 -- score: 0.690909090909091[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  26 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 22:31:49] Features: 6/10 -- score: 0.6939393939393941[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  25 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 22:31:50] Features: 7/10 -- score: 0.6941348973607038[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  24 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 22:31:50] Features: 8/10 -- score: 0.6971652003910069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  23 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 22:31:51] Features: 9/10 -- score: 0.703225806451613[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  22 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    0.5s finished\n",
      "\n",
      "[2019-03-19 22:31:51] Features: 10/10 -- score: 0.7066471163245358"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'feature_idx': (15,),\n",
       "  'cv_scores': array([0.63636364, 0.57575758, 0.66666667, 0.67741935, 0.67741935,\n",
       "         0.64516129, 0.77419355, 0.67741935, 0.64516129, 0.67741935]),\n",
       "  'avg_score': 0.6652981427174975,\n",
       "  'feature_names': ('15',)},\n",
       " 2: {'feature_idx': (15, 16),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.6969697 , 0.70967742, 0.74193548,\n",
       "         0.58064516, 0.83870968, 0.64516129, 0.70967742, 0.64516129]),\n",
       "  'avg_score': 0.6749755620723363,\n",
       "  'feature_names': ('15', '16')},\n",
       " 3: {'feature_idx': (15, 16, 20),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.72727273, 0.74193548, 0.77419355,\n",
       "         0.58064516, 0.83870968, 0.64516129, 0.67741935, 0.67741935]),\n",
       "  'avg_score': 0.6844574780058652,\n",
       "  'feature_names': ('15', '16', '20')},\n",
       " 4: {'feature_idx': (7, 15, 16, 20),\n",
       "  'cv_scores': array([0.57575758, 0.63636364, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.61290323, 0.67741935, 0.70967742]),\n",
       "  'avg_score': 0.6874877810361683,\n",
       "  'feature_names': ('7', '15', '16', '20')},\n",
       " 5: {'feature_idx': (7, 15, 16, 20, 22),\n",
       "  'cv_scores': array([0.57575758, 0.60606061, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.690909090909091,\n",
       "  'feature_names': ('7', '15', '16', '20', '22')},\n",
       " 6: {'feature_idx': (1, 7, 15, 16, 20, 22),\n",
       "  'cv_scores': array([0.57575758, 0.63636364, 0.72727273, 0.77419355, 0.77419355,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6939393939393941,\n",
       "  'feature_names': ('1', '7', '15', '16', '20', '22')},\n",
       " 7: {'feature_idx': (1, 7, 15, 16, 20, 21, 22),\n",
       "  'cv_scores': array([0.54545455, 0.63636364, 0.72727273, 0.77419355, 0.80645161,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6941348973607038,\n",
       "  'feature_names': ('1', '7', '15', '16', '20', '21', '22')},\n",
       " 8: {'feature_idx': (1, 7, 12, 15, 16, 20, 21, 22),\n",
       "  'cv_scores': array([0.60606061, 0.63636364, 0.6969697 , 0.77419355, 0.80645161,\n",
       "         0.58064516, 0.80645161, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6971652003910069,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '20', '21', '22')},\n",
       " 9: {'feature_idx': (1, 7, 12, 15, 16, 20, 21, 22, 26),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.66666667, 0.77419355, 0.80645161,\n",
       "         0.61290323, 0.77419355, 0.61290323, 0.67741935, 0.77419355]),\n",
       "  'avg_score': 0.703225806451613,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '20', '21', '22', '26')},\n",
       " 10: {'feature_idx': (1, 7, 12, 15, 16, 18, 20, 21, 22, 26),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.63636364, 0.77419355, 0.80645161,\n",
       "         0.61290323, 0.80645161, 0.64516129, 0.67741935, 0.77419355]),\n",
       "  'avg_score': 0.7066471163245358,\n",
       "  'feature_names': ('1', '7', '12', '15', '16', '18', '20', '21', '22', '26')}}"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs = sffs.fit(X_train,y_train)\n",
    "sffs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7066471163245358"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sffs.k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5822784810126582\n",
      " - precision = 0.5283018867924528\n",
      " - recall = 0.7777777777777778\n",
      " - f1 = 0.6292134831460674\n"
     ]
    }
   ],
   "source": [
    "X_train_model = sffs.transform(X_train)\n",
    "X_train_model\n",
    "X_test_model = sffs.transform(X_test)\n",
    "X_test_model\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.4936708860759494\n",
      " - precision = 0.45454545454545453\n",
      " - recall = 0.5555555555555556\n",
      " - f1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5063291139240507\n",
      " - precision = 0.47058823529411764\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XGBOOT:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 5ms/step - loss: 0.6929 - acc: 0.5348\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6924 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.6913 - acc: 0.5696\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6886 - acc: 0.6456\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6837 - acc: 0.6899\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.6749 - acc: 0.6930\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6637 - acc: 0.6899\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6516 - acc: 0.6867\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6448 - acc: 0.6171\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.6372 - acc: 0.6835\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.6306 - acc: 0.6930\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.6271 - acc: 0.6835\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6242 - acc: 0.6899\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.6218 - acc: 0.6930\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.6200 - acc: 0.6930\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.6193 - acc: 0.6899\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.6162 - acc: 0.6962\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.6143 - acc: 0.7057\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 208us/step - loss: 0.6127 - acc: 0.7057\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.6111 - acc: 0.6994\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.6089 - acc: 0.7120\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.6084 - acc: 0.7025\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6060 - acc: 0.7120\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.6051 - acc: 0.6994\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6039 - acc: 0.6994\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.6032 - acc: 0.6994\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 205us/step - loss: 0.6033 - acc: 0.6930\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 229us/step - loss: 0.6002 - acc: 0.7120\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.5987 - acc: 0.7089\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 232us/step - loss: 0.5976 - acc: 0.7057\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 231us/step - loss: 0.5981 - acc: 0.7025\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 222us/step - loss: 0.5975 - acc: 0.7057\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 228us/step - loss: 0.5959 - acc: 0.7057\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5949 - acc: 0.7025\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 195us/step - loss: 0.5938 - acc: 0.7057\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 210us/step - loss: 0.5919 - acc: 0.7089\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5931 - acc: 0.7089\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5912 - acc: 0.6994\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 210us/step - loss: 0.5915 - acc: 0.7120\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 214us/step - loss: 0.5897 - acc: 0.7120\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 208us/step - loss: 0.5888 - acc: 0.7184\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5883 - acc: 0.7025\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5885 - acc: 0.6994\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5875 - acc: 0.7089\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5874 - acc: 0.7025\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5889 - acc: 0.7057\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5862 - acc: 0.6994\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5843 - acc: 0.7089\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5856 - acc: 0.7247\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5839 - acc: 0.7057\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5828 - acc: 0.7089\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5814 - acc: 0.7152\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5811 - acc: 0.7089\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5799 - acc: 0.7120\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5801 - acc: 0.7120\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5800 - acc: 0.7184\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5808 - acc: 0.7057\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5789 - acc: 0.7184\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5790 - acc: 0.7089\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5769 - acc: 0.7184\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5765 - acc: 0.7152\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5806 - acc: 0.7184\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.5782 - acc: 0.7120\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 207us/step - loss: 0.5774 - acc: 0.7025\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5790 - acc: 0.7247\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5750 - acc: 0.7120\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5754 - acc: 0.7215\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5743 - acc: 0.7215\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5745 - acc: 0.7120\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5745 - acc: 0.7152\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5735 - acc: 0.7152\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.5726 - acc: 0.7215\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5752 - acc: 0.7025\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5752 - acc: 0.7184\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5746 - acc: 0.7120\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5708 - acc: 0.7184\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5729 - acc: 0.7152\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5723 - acc: 0.7120\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5700 - acc: 0.7184\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5696 - acc: 0.7215\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5709 - acc: 0.7152\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.5697 - acc: 0.7152\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5687 - acc: 0.7089\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.5702 - acc: 0.7247\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5680 - acc: 0.7247\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5686 - acc: 0.7247\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5699 - acc: 0.7120\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5683 - acc: 0.7215\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5669 - acc: 0.7089\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.5670 - acc: 0.7089\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 224us/step - loss: 0.5658 - acc: 0.7184\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 208us/step - loss: 0.5661 - acc: 0.7025\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.5663 - acc: 0.7215\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5650 - acc: 0.7184\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5694 - acc: 0.7089\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5654 - acc: 0.7247\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5669 - acc: 0.7247\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.5639 - acc: 0.7184\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 242us/step - loss: 0.5654 - acc: 0.7120\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 214us/step - loss: 0.5642 - acc: 0.7342\n",
      "79/79 [==============================] - 1s 7ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.6959179043769836\n",
      "Test accuracy: 0.5443037748336792\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SBS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Set 'forward' to False for backward feature selection\n",
    "sbfs = SBS(logisticRegr,\n",
    "k_features=10,\n",
    "forward=False, #For backward selection, set it to False\n",
    "floating=False,\n",
    "verbose=2,\n",
    "scoring='accuracy',\n",
    "cv=10,#10-cross validation\n",
    "n_jobs=-1)#Using all availabel CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    4.0s finished\n",
      "\n",
      "[2019-03-19 22:32:27] Features: 30/10 -- score: 0.6689149560117302[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.6s finished\n",
      "\n",
      "[2019-03-19 22:32:31] Features: 29/10 -- score: 0.6789833822091886[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    3.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    3.8s finished\n",
      "\n",
      "[2019-03-19 22:32:35] Features: 28/10 -- score: 0.6852394916911047[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    3.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    3.5s finished\n",
      "\n",
      "[2019-03-19 22:32:38] Features: 27/10 -- score: 0.6882697947214076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    3.4s finished\n",
      "\n",
      "[2019-03-19 22:32:42] Features: 26/10 -- score: 0.6882697947214076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    2.8s finished\n",
      "\n",
      "[2019-03-19 22:32:45] Features: 25/10 -- score: 0.6913000977517108[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    2.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    2.4s finished\n",
      "\n",
      "[2019-03-19 22:32:47] Features: 24/10 -- score: 0.70019550342131[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    2.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    2.2s finished\n",
      "\n",
      "[2019-03-19 22:32:49] Features: 23/10 -- score: 0.7068426197458456[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  23 | elapsed:    2.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    2.1s finished\n",
      "\n",
      "[2019-03-19 22:32:51] Features: 22/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  22 | elapsed:    2.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    2.1s finished\n",
      "\n",
      "[2019-03-19 22:32:53] Features: 21/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  21 | elapsed:    1.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    1.8s finished\n",
      "\n",
      "[2019-03-19 22:32:55] Features: 20/10 -- score: 0.7161290322580646[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    1.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    2.2s finished\n",
      "\n",
      "[2019-03-19 22:32:57] Features: 19/10 -- score: 0.7191593352883676[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  19 | elapsed:    1.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    1.8s finished\n",
      "\n",
      "[2019-03-19 22:32:59] Features: 18/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:    1.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.5s finished\n",
      "\n",
      "[2019-03-19 22:33:01] Features: 17/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  17 | elapsed:    0.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    1.0s finished\n",
      "\n",
      "[2019-03-19 22:33:02] Features: 16/10 -- score: 0.7189638318670577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  16 | elapsed:    0.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    1.0s finished\n",
      "\n",
      "[2019-03-19 22:33:03] Features: 15/10 -- score: 0.7159335288367548[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.9s finished\n",
      "\n",
      "[2019-03-19 22:33:04] Features: 14/10 -- score: 0.7159335288367548[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    0.8s finished\n",
      "\n",
      "[2019-03-19 22:33:05] Features: 13/10 -- score: 0.7187683284457479[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  13 | elapsed:    0.5s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    0.8s finished\n",
      "\n",
      "[2019-03-19 22:33:05] Features: 12/10 -- score: 0.7155425219941349[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  12 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "\n",
      "[2019-03-19 22:33:06] Features: 11/10 -- score: 0.7036168132942328[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed:    0.2s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.3s finished\n",
      "\n",
      "[2019-03-19 22:33:06] Features: 10/10 -- score: 0.70019550342131"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{31: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.48484848, 0.61290323, 0.74193548,\n",
       "         0.5483871 , 0.87096774, 0.58064516, 0.67741935, 0.61290323]),\n",
       "  'avg_score': 0.6402737047898339,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '17',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 30: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.58064516, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.64516129, 0.64516129, 0.70967742]),\n",
       "  'avg_score': 0.6689149560117302,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '23',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 29: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.48484848, 0.64516129, 0.80645161,\n",
       "         0.64516129, 0.83870968, 0.64516129, 0.70967742, 0.74193548]),\n",
       "  'avg_score': 0.6789833822091886,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '22',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 28: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.60606061, 0.63636364, 0.54545455, 0.64516129, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.70967742, 0.74193548]),\n",
       "  'avg_score': 0.6852394916911047,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '8',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 27: {'feature_idx': (0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.67741935, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6882697947214076,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 26: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.54545455, 0.67741935, 0.80645161,\n",
       "         0.67741935, 0.83870968, 0.64516129, 0.67741935, 0.74193548]),\n",
       "  'avg_score': 0.6882697947214076,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '20',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 25: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.60606061, 0.60606061, 0.70967742, 0.74193548,\n",
       "         0.61290323, 0.83870968, 0.64516129, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.6913000977517108,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '13',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 24: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.66666667, 0.66666667, 0.67741935, 0.77419355,\n",
       "         0.64516129, 0.83870968, 0.64516129, 0.74193548, 0.70967742]),\n",
       "  'avg_score': 0.70019550342131,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '5',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 23: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.66666667, 0.70967742, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7068426197458456,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '14',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 22: {'feature_idx': (0,\n",
       "   1,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.63636364, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '1',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 21: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   18,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.63636364, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '18',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 20: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.66666667, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7161290322580646,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '11',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 19: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.64516129, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7191593352883676,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '7',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 18: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '21',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 17: {'feature_idx': (0,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('0',\n",
       "   '3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 16: {'feature_idx': (3,\n",
       "   4,\n",
       "   6,\n",
       "   9,\n",
       "   10,\n",
       "   12,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30),\n",
       "  'cv_scores': array([0.66666667, 0.66666667, 0.72727273, 0.70967742, 0.77419355,\n",
       "         0.67741935, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7189638318670577,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '19',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 15: {'feature_idx': (3, 4, 6, 9, 10, 12, 15, 16, 24, 25, 26, 27, 28, 29, 30),\n",
       "  'cv_scores': array([0.6969697 , 0.60606061, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7159335288367548,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '29',\n",
       "   '30')},\n",
       " 14: {'feature_idx': (3, 4, 6, 9, 10, 12, 15, 16, 24, 25, 26, 27, 28, 30),\n",
       "  'cv_scores': array([0.6969697 , 0.60606061, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.64516129, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7159335288367548,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '10',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '30')},\n",
       " 13: {'feature_idx': (3, 4, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28, 30),\n",
       "  'cv_scores': array([0.72727273, 0.63636364, 0.72727273, 0.70967742, 0.80645161,\n",
       "         0.61290323, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7187683284457479,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28',\n",
       "   '30')},\n",
       " 12: {'feature_idx': (3, 4, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.72727273, 0.66666667, 0.6969697 , 0.70967742, 0.77419355,\n",
       "         0.61290323, 0.87096774, 0.61290323, 0.74193548, 0.74193548]),\n",
       "  'avg_score': 0.7155425219941349,\n",
       "  'feature_names': ('3',\n",
       "   '4',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28')},\n",
       " 11: {'feature_idx': (3, 6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.63636364, 0.63636364, 0.66666667, 0.74193548, 0.77419355,\n",
       "         0.58064516, 0.83870968, 0.61290323, 0.74193548, 0.80645161]),\n",
       "  'avg_score': 0.7036168132942328,\n",
       "  'feature_names': ('3',\n",
       "   '6',\n",
       "   '9',\n",
       "   '12',\n",
       "   '15',\n",
       "   '16',\n",
       "   '24',\n",
       "   '25',\n",
       "   '26',\n",
       "   '27',\n",
       "   '28')},\n",
       " 10: {'feature_idx': (6, 9, 12, 15, 16, 24, 25, 26, 27, 28),\n",
       "  'cv_scores': array([0.63636364, 0.66666667, 0.66666667, 0.70967742, 0.74193548,\n",
       "         0.58064516, 0.87096774, 0.61290323, 0.74193548, 0.77419355]),\n",
       "  'avg_score': 0.70019550342131,\n",
       "  'feature_names': ('6', '9', '12', '15', '16', '24', '25', '26', '27', '28')}}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbfs = sbfs.fit(X_train, y_train)\n",
    "sbfs.subsets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70019550342131"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbfs.k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance:\n",
      " - accuracy = 0.5822784810126582\n",
      " - precision = 0.5306122448979592\n",
      " - recall = 0.7222222222222222\n",
      " - f1 = 0.611764705882353\n"
     ]
    }
   ],
   "source": [
    "X_train_model = sbfs.transform(X_train)\n",
    "X_test_model = sbfs.transform(X_test)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5189873417721519\n",
      " - precision = 0.4791666666666667\n",
      " - recall = 0.6388888888888888\n",
      " - f1 = 0.5476190476190476\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5111111111111111\n",
      " - recall = 0.6388888888888888\n",
      " - f1 = 0.5679012345679012\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XGBOOT:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 5ms/step - loss: 0.6929 - acc: 0.5348\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.6923 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.6918 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.6904 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.6884 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.6854 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6813 - acc: 0.5475\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.6759 - acc: 0.6044\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.6705 - acc: 0.6108\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6613 - acc: 0.6329\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.6521 - acc: 0.6551\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.6403 - acc: 0.6614\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6329 - acc: 0.6772\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 173us/step - loss: 0.6213 - acc: 0.6772\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6129 - acc: 0.6741\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.6062 - acc: 0.6677\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6012 - acc: 0.6867\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5972 - acc: 0.6867\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 210us/step - loss: 0.5916 - acc: 0.6772\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.5869 - acc: 0.6709\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 221us/step - loss: 0.5839 - acc: 0.6899\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 211us/step - loss: 0.5815 - acc: 0.6835\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5793 - acc: 0.6899\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5786 - acc: 0.7025\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5795 - acc: 0.7057\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5766 - acc: 0.6930\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5772 - acc: 0.6772\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5857 - acc: 0.6835\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5747 - acc: 0.6930\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5732 - acc: 0.6930\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5727 - acc: 0.6962\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5763 - acc: 0.6994\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5739 - acc: 0.6962\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.5740 - acc: 0.6930\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5732 - acc: 0.7025\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5693 - acc: 0.6994\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 214us/step - loss: 0.5698 - acc: 0.6835\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5736 - acc: 0.6930\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 223us/step - loss: 0.5698 - acc: 0.6994\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5756 - acc: 0.6994\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5742 - acc: 0.6835\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5755 - acc: 0.6994\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5703 - acc: 0.6930\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5717 - acc: 0.6772\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5701 - acc: 0.7057\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5707 - acc: 0.6962\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5692 - acc: 0.6994\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 209us/step - loss: 0.5704 - acc: 0.6867\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5698 - acc: 0.6994\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5699 - acc: 0.6899\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5709 - acc: 0.6899\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5714 - acc: 0.6867\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5693 - acc: 0.7057\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5690 - acc: 0.6899\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5677 - acc: 0.6962\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.5702 - acc: 0.6835\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5685 - acc: 0.6962\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5681 - acc: 0.7089\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5672 - acc: 0.7057\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5670 - acc: 0.7025\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5691 - acc: 0.6930\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5708 - acc: 0.7120\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 209us/step - loss: 0.5699 - acc: 0.7057\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 216us/step - loss: 0.5694 - acc: 0.6994\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 219us/step - loss: 0.5688 - acc: 0.7025\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.5695 - acc: 0.6930\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.5700 - acc: 0.7089\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5705 - acc: 0.6899\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5693 - acc: 0.6962\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5662 - acc: 0.6962\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5707 - acc: 0.6899\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5673 - acc: 0.6930\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5655 - acc: 0.7057\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 211us/step - loss: 0.5672 - acc: 0.7025\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5652 - acc: 0.6899\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 214us/step - loss: 0.5697 - acc: 0.6930\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5667 - acc: 0.7057\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5670 - acc: 0.6899\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5674 - acc: 0.6930\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 203us/step - loss: 0.5662 - acc: 0.7025\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5668 - acc: 0.7057\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5677 - acc: 0.7057\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5674 - acc: 0.7057\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5674 - acc: 0.7057\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5711 - acc: 0.6962\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5690 - acc: 0.6962\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5659 - acc: 0.7025\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5655 - acc: 0.6962\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5679 - acc: 0.6994\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5655 - acc: 0.7057\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5717 - acc: 0.7089\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5653 - acc: 0.6994\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5699 - acc: 0.6930\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5669 - acc: 0.7025\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5667 - acc: 0.6930\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5698 - acc: 0.7152\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5652 - acc: 0.6899\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5656 - acc: 0.6930\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5651 - acc: 0.7120\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5663 - acc: 0.7089\n",
      "79/79 [==============================] - 1s 8ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.7557664513587952\n",
      "Test accuracy: 0.5696202516555786\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECURSIVE AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "rfe = RFE(estimator=logisticRegr, n_features_to_select=10)\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.5949367088607594\n",
      " - precision = 0.54\n",
      " - recall = 0.75\n",
      " - f1 = 0.627906976744186\n"
     ]
    }
   ],
   "source": [
    "X_train_model = rfe.transform(X_train)\n",
    "X_test_model = rfe.transform(X_test)\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5569620253164557\n",
      " - precision = 0.5121951219512195\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.6075949367088608\n",
      " - precision = 0.5581395348837209\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XGBOOT:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 5ms/step - loss: 0.6929 - acc: 0.5475\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6921 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6908 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6887 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.6850 - acc: 0.5475\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.6806 - acc: 0.5475\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6758 - acc: 0.5633\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.6685 - acc: 0.6044\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.6628 - acc: 0.6171\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.6533 - acc: 0.6614\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6446 - acc: 0.6582\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6341 - acc: 0.6646\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.6261 - acc: 0.6741\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 215us/step - loss: 0.6162 - acc: 0.6741\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.6059 - acc: 0.6741\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 221us/step - loss: 0.5983 - acc: 0.6899\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5945 - acc: 0.6835\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5867 - acc: 0.6677\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5847 - acc: 0.6835\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5806 - acc: 0.6804\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.5778 - acc: 0.6804\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5778 - acc: 0.6899\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5756 - acc: 0.6804\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5717 - acc: 0.6867\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5725 - acc: 0.6867\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5726 - acc: 0.6772\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5761 - acc: 0.6677\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5804 - acc: 0.6804\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5713 - acc: 0.6835\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 195us/step - loss: 0.5761 - acc: 0.6709\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 207us/step - loss: 0.5705 - acc: 0.6867\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5695 - acc: 0.6930\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 204us/step - loss: 0.5700 - acc: 0.6772\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5696 - acc: 0.6930\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5692 - acc: 0.6772\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5697 - acc: 0.6772\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5717 - acc: 0.6804\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5726 - acc: 0.6899\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5678 - acc: 0.6930\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5670 - acc: 0.6962\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.5699 - acc: 0.6835\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 195us/step - loss: 0.5696 - acc: 0.6741\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5662 - acc: 0.6962\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5683 - acc: 0.6930\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5667 - acc: 0.6867\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5664 - acc: 0.6804\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5664 - acc: 0.6867\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 201us/step - loss: 0.5712 - acc: 0.6804\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 243us/step - loss: 0.5672 - acc: 0.6804\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5665 - acc: 0.6741\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5671 - acc: 0.6772\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5684 - acc: 0.6772\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5660 - acc: 0.6899\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.5662 - acc: 0.6962\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5686 - acc: 0.6772\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5668 - acc: 0.6867\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5707 - acc: 0.6835\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5732 - acc: 0.6772\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5661 - acc: 0.6835\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5652 - acc: 0.6804\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 216us/step - loss: 0.5652 - acc: 0.6899\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5696 - acc: 0.6835\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5630 - acc: 0.6772\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.5684 - acc: 0.6899\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5659 - acc: 0.6899\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 217us/step - loss: 0.5670 - acc: 0.6899\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5704 - acc: 0.6709\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 198us/step - loss: 0.5671 - acc: 0.6962\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 250us/step - loss: 0.5649 - acc: 0.6835\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 213us/step - loss: 0.5658 - acc: 0.6804\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 214us/step - loss: 0.5643 - acc: 0.6962\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5639 - acc: 0.6962\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5639 - acc: 0.6835\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5644 - acc: 0.6804\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 206us/step - loss: 0.5651 - acc: 0.6930\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 215us/step - loss: 0.5634 - acc: 0.6804\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 215us/step - loss: 0.5673 - acc: 0.6772\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5698 - acc: 0.6835\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5661 - acc: 0.6899\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5668 - acc: 0.6867\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5645 - acc: 0.6804\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5656 - acc: 0.6867\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5663 - acc: 0.6867\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.5657 - acc: 0.6741\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5637 - acc: 0.6835\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5639 - acc: 0.6804\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5735 - acc: 0.6772\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 194us/step - loss: 0.5654 - acc: 0.6772\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5654 - acc: 0.6804\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5651 - acc: 0.6835\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5655 - acc: 0.6741\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5631 - acc: 0.6741\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 196us/step - loss: 0.5644 - acc: 0.6772\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5636 - acc: 0.6804\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5657 - acc: 0.6835\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5642 - acc: 0.6835\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5683 - acc: 0.6804\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5682 - acc: 0.6962\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5638 - acc: 0.6867\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5653 - acc: 0.6677\n",
      "79/79 [==============================] - 1s 8ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.7310895323753357\n",
      "Test accuracy: 0.6202531456947327\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 10, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded Methods (Tree-Based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=30434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=30434, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01380813 0.01353761 0.00723804 0.02454823 0.04799143 0.01357749\n",
      " 0.02113829 0.00928241 0.04662204 0.04653403 0.05033548 0.03596542\n",
      " 0.03949406 0.02788433 0.04028098 0.08653448 0.02094389 0.01745103\n",
      " 0.02138969 0.0164613  0.01315435 0.01147229 0.01673737 0.0196627\n",
      " 0.03942599 0.05573863 0.05791559 0.02723297 0.04968925 0.04343057\n",
      " 0.06452194]\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=30434, verbose=0,\n",
       "            warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=0.04)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "sfm = SelectFromModel(rf, threshold=0.04)\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False, False, False,  True,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feature_boolean_selector = sfm.get_support()\n",
    "rf_feature_boolean_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_model = sfm.transform(X_train)\n",
    "X_test_model = sfm.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train_model, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.4430379746835443\n",
      " - precision = 0.43333333333333335\n",
      " - recall = 0.7222222222222222\n",
      " - f1 = 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.569620253164557\n",
      " - precision = 0.525\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5316455696202531\n",
      " - precision = 0.4883720930232558\n",
      " - recall = 0.5833333333333334\n",
      " - f1 = 0.5316455696202531\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 5ms/step - loss: 0.6930 - acc: 0.5411\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6927 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6922 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.6917 - acc: 0.5475\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6906 - acc: 0.5601\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6893 - acc: 0.5918\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6857 - acc: 0.6456\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6815 - acc: 0.6487\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6762 - acc: 0.6487\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6689 - acc: 0.6519\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.6628 - acc: 0.6329\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6567 - acc: 0.6171\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6506 - acc: 0.6646\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.6437 - acc: 0.6614\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6374 - acc: 0.6519\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6353 - acc: 0.6614\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6291 - acc: 0.6614\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 202us/step - loss: 0.6260 - acc: 0.6804\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 233us/step - loss: 0.6232 - acc: 0.6551\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.6230 - acc: 0.6614\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6147 - acc: 0.6804\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.6124 - acc: 0.6741\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.6138 - acc: 0.6646\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6102 - acc: 0.6835\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.6084 - acc: 0.6867\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 193us/step - loss: 0.6068 - acc: 0.6709\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.6111 - acc: 0.6741\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.6035 - acc: 0.6867\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.6031 - acc: 0.6772\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6028 - acc: 0.6899\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6042 - acc: 0.6646\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 0.6033 - acc: 0.6835\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 150us/step - loss: 0.6023 - acc: 0.6772\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6001 - acc: 0.6709\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.6015 - acc: 0.6709\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6009 - acc: 0.6899\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 192us/step - loss: 0.5965 - acc: 0.6804\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5960 - acc: 0.6835\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5964 - acc: 0.6835\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5963 - acc: 0.6677\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 0.5951 - acc: 0.6835\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5945 - acc: 0.6835\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5955 - acc: 0.6741\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5933 - acc: 0.6772\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 186us/step - loss: 0.5949 - acc: 0.6835\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.5936 - acc: 0.6772\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 199us/step - loss: 0.5976 - acc: 0.6835\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5959 - acc: 0.6677\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 197us/step - loss: 0.5939 - acc: 0.6677\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 191us/step - loss: 0.5947 - acc: 0.6741\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5918 - acc: 0.6772\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5910 - acc: 0.6772\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5931 - acc: 0.6772\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5919 - acc: 0.6899\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5921 - acc: 0.6677\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5896 - acc: 0.6772\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5938 - acc: 0.6930\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5891 - acc: 0.6741\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5919 - acc: 0.6804\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5909 - acc: 0.6772\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5934 - acc: 0.6741\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5904 - acc: 0.6772\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5886 - acc: 0.6835\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5923 - acc: 0.6709\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5941 - acc: 0.6867\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5890 - acc: 0.6772\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 205us/step - loss: 0.5868 - acc: 0.6709\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5902 - acc: 0.6930\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.5884 - acc: 0.6677\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5906 - acc: 0.6962\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5878 - acc: 0.6709\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5876 - acc: 0.6804\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5888 - acc: 0.6899\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5871 - acc: 0.6741\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5937 - acc: 0.6709\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5866 - acc: 0.6867\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5893 - acc: 0.6709\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5853 - acc: 0.6962\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5846 - acc: 0.6835\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.5849 - acc: 0.6835\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5848 - acc: 0.6962\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5872 - acc: 0.6804\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5861 - acc: 0.6835\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5851 - acc: 0.6677\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5876 - acc: 0.6899\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.5859 - acc: 0.6835\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5847 - acc: 0.6835\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5836 - acc: 0.6804\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5854 - acc: 0.6804\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5864 - acc: 0.6772\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5838 - acc: 0.6899\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5830 - acc: 0.6804\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5814 - acc: 0.6899\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.5857 - acc: 0.6677\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5844 - acc: 0.6867\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5815 - acc: 0.6835\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 151us/step - loss: 0.5844 - acc: 0.6962\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5830 - acc: 0.6804\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5813 - acc: 0.6899\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 205us/step - loss: 0.5824 - acc: 0.6899\n",
      "79/79 [==============================] - 1s 8ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.6985602378845215\n",
      "Test accuracy: 0.5822784900665283\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 11, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=11)\n",
    "pca_selector= pca.fit(X_train,y_train)\n",
    "X_train_model = pca_selector.transform(X_train)\n",
    "X_test_model = pca_selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Logistic Regression:\n",
      " - accuracy = 0.4430379746835443\n",
      " - precision = 0.43333333333333335\n",
      " - recall = 0.7222222222222222\n",
      " - f1 = 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "predictions = logisticRegr.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Logistic Regression:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Random forest:\n",
      " - accuracy = 0.5189873417721519\n",
      " - precision = 0.48\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5581395348837209\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators = 100, random_state=30434)\n",
    "clf.fit(X_train_model,y_train)\n",
    "predictions=clf.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance Random forest:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance XgBoost:\n",
      " - accuracy = 0.5443037974683544\n",
      " - precision = 0.5\n",
      " - recall = 0.6666666666666666\n",
      " - f1 = 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "# XGBOOT\n",
    "import xgboost as xgb\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_model, y_train)\n",
    "predictions = gbm.predict(X_test_model)\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"Performance XgBoost:\")\n",
    "print(\" - accuracy = \" + str(accuracy))\n",
    "print(\" - precision = \" + str(precision))\n",
    "print(\" - recall = \" + str(recall))\n",
    "print(\" - f1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "316/316 [==============================] - 2s 6ms/step - loss: 0.6928 - acc: 0.5443\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.6922 - acc: 0.5475\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6915 - acc: 0.5475\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6899 - acc: 0.5506\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 173us/step - loss: 0.6877 - acc: 0.5538\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6836 - acc: 0.5759\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.6783 - acc: 0.5918\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6707 - acc: 0.5918\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6632 - acc: 0.6234\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.6559 - acc: 0.6203\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6487 - acc: 0.6044\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.6432 - acc: 0.6234\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 156us/step - loss: 0.6383 - acc: 0.6329\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.6350 - acc: 0.6266\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.6309 - acc: 0.6361\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.6282 - acc: 0.6424\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6247 - acc: 0.6424\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6242 - acc: 0.6392\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6205 - acc: 0.6487\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6172 - acc: 0.6487\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.6181 - acc: 0.6361\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.6139 - acc: 0.6424\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.6138 - acc: 0.6519\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.6103 - acc: 0.6456\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6085 - acc: 0.6456\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 169us/step - loss: 0.6079 - acc: 0.6424\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6071 - acc: 0.6392\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 177us/step - loss: 0.6050 - acc: 0.6329\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.6031 - acc: 0.6424\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.6031 - acc: 0.6424\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.6012 - acc: 0.6424\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5990 - acc: 0.6392\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.5982 - acc: 0.6392\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5967 - acc: 0.6582\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5966 - acc: 0.6551\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5965 - acc: 0.6582\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 178us/step - loss: 0.5950 - acc: 0.6582\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 200us/step - loss: 0.5929 - acc: 0.6614\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 190us/step - loss: 0.5910 - acc: 0.6646\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5915 - acc: 0.6614\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 189us/step - loss: 0.5893 - acc: 0.6646\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 195us/step - loss: 0.5886 - acc: 0.6709\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5886 - acc: 0.6709\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 188us/step - loss: 0.5865 - acc: 0.6741\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5869 - acc: 0.6709\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 170us/step - loss: 0.5850 - acc: 0.6677\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 187us/step - loss: 0.5874 - acc: 0.6646\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 182us/step - loss: 0.5845 - acc: 0.6646\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 172us/step - loss: 0.5826 - acc: 0.6677\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 163us/step - loss: 0.5819 - acc: 0.6646\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5810 - acc: 0.6646\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5800 - acc: 0.6646\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5780 - acc: 0.6677\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5787 - acc: 0.6646\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5766 - acc: 0.6741\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 152us/step - loss: 0.5763 - acc: 0.6741\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 0.5766 - acc: 0.6646\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5760 - acc: 0.6709\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5738 - acc: 0.6677\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5754 - acc: 0.6772\n",
      "Epoch 61/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5763 - acc: 0.6709\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5728 - acc: 0.6741\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5717 - acc: 0.6835\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 180us/step - loss: 0.5712 - acc: 0.6804\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 162us/step - loss: 0.5712 - acc: 0.6772\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5690 - acc: 0.6804\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 0.5701 - acc: 0.6835\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 168us/step - loss: 0.5681 - acc: 0.6804\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5673 - acc: 0.6835\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 145us/step - loss: 0.5664 - acc: 0.6899\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 0.5652 - acc: 0.6835\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 0.5658 - acc: 0.6899\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 175us/step - loss: 0.5641 - acc: 0.6962\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 184us/step - loss: 0.5661 - acc: 0.6962\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 185us/step - loss: 0.5623 - acc: 0.6994\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 174us/step - loss: 0.5627 - acc: 0.6930\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5616 - acc: 0.6899\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 181us/step - loss: 0.5635 - acc: 0.7025\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5620 - acc: 0.6994\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5605 - acc: 0.7025\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5578 - acc: 0.6899\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5581 - acc: 0.6930\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5568 - acc: 0.7025\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5561 - acc: 0.6994\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 167us/step - loss: 0.5536 - acc: 0.7120\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 153us/step - loss: 0.5534 - acc: 0.7089\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 176us/step - loss: 0.5520 - acc: 0.7025\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 166us/step - loss: 0.5521 - acc: 0.7089\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 173us/step - loss: 0.5495 - acc: 0.7152\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 179us/step - loss: 0.5493 - acc: 0.7089\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 171us/step - loss: 0.5472 - acc: 0.7152\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5476 - acc: 0.7152\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 160us/step - loss: 0.5471 - acc: 0.7120\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5461 - acc: 0.7152\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 161us/step - loss: 0.5450 - acc: 0.7184\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5445 - acc: 0.7184\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5415 - acc: 0.7278\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 164us/step - loss: 0.5416 - acc: 0.7247\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 165us/step - loss: 0.5416 - acc: 0.7247\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 158us/step - loss: 0.5404 - acc: 0.7120\n",
      "79/79 [==============================] - 1s 8ms/step\n",
      "Performance Neuron network:\n",
      "Test score: 0.8487426042556763\n",
      "Test accuracy: 0.5316455960273743\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier = Sequential()\n",
    " # Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting ANN to the training set\n",
    "classifier.fit(X_train_model, y_train, batch_size = 11, nb_epoch = 100)\n",
    "# predictions = classifier.predict(X_test_model)\n",
    "score = classifier.evaluate(X_test_model, y_test, batch_size=128)\n",
    "print(\"Performance Neuron network:\")\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
